{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> NLP - TEXT MININING PROJECT | Fernando Llopis Bernal </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerias que voy a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim import models\n",
    "import itertools\n",
    "\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, configuro los parámetros para la visualización de los varios dataframes en mi PyCharm, que es donde lo he desarrollado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe visualization parameters\n",
    "desired_width = 320\n",
    "pd.set_option('display.width', desired_width)\n",
    "np.set_printoptions(linewidth=desired_width)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/Users/iojeda/Downloads/train.csv')\n",
    "test_df = pd.read_csv('C:/Users/iojeda/Downloads/test.csv')\n",
    "sample_df = pd.read_csv('C:/Users/iojeda/Downloads/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                                                                                 text  target\n",
       "0   1     NaN      NaN                                Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all       1\n",
       "1   4     NaN      NaN                                                               Forest fire near La Ronge Sask. Canada       1\n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...       1\n",
       "3   6     NaN      NaN                                    13,000 people receive #wildfires evacuation orders in California        1\n",
       "4   7     NaN      NaN             Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school        1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7613 non-null int64\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000</td>\n",
       "      <td>7613.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.935</td>\n",
       "      <td>0.430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116</td>\n",
       "      <td>0.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id   target\n",
       "count  7613.000 7613.000\n",
       "mean   5441.935    0.430\n",
       "std    3137.116    0.495\n",
       "min       1.000    0.000\n",
       "25%    2734.000    0.000\n",
       "50%    5408.000    0.000\n",
       "75%    8146.000    1.000\n",
       "max   10873.000    1.000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                                                                              text\n",
       "0   0     NaN      NaN                                                                Just happened a terrible car crash\n",
       "1   2     NaN      NaN                                  Heard about #earthquake is different cities, stay safe everyone.\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all\n",
       "3   9     NaN      NaN                                                          Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN                                                     Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      "id          3263 non-null int64\n",
      "keyword     3237 non-null object\n",
      "location    2158 non-null object\n",
      "text        3263 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3263.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5427.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3146.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2683.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5500.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8176.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10875.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id\n",
       "count  3263.000\n",
       "mean   5427.153\n",
       "std    3146.427\n",
       "min       0.000\n",
       "25%    2683.000\n",
       "50%    5500.000\n",
       "75%    8176.000\n",
       "max   10875.000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que he pretendido a continuación, creando dos dataframes para realizar un análisis exploratorio, es ver si podrían ser representativos sólo aquellas observaciones que contengan en última instancia, ambas observaciones (no NaN, NA, ...) y posteriormente ver si uso esos subsets o ambas poblaciones enteras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nas keyword:  61\n",
      "nas location:  2533\n"
     ]
    }
   ],
   "source": [
    "nas_keyword = sum(pd.isnull(train_df['keyword']))\n",
    "print('nas keyword: ', nas_keyword)\n",
    "nas_location = sum((pd.isnull(train_df['location'])))\n",
    "print('nas location: ', nas_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nas keyword:  26\n",
      "nas location:  1105\n"
     ]
    }
   ],
   "source": [
    "nas_keyword = sum(pd.isnull(test_df['keyword']))\n",
    "print('nas keyword: ',nas_keyword)\n",
    "nas_location = sum((pd.isnull(test_df['location'])))\n",
    "print('nas location: ', nas_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set with all observations containing any valid keyword\n",
      "    index  id keyword                       location                                                                                text  target\n",
      "0     31  48  ablaze                     Birmingham                             @bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C       1\n",
      "1     32  49  ablaze  Est. September 2012 - Bristol                 We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw       0\n",
      "2     33  50  ablaze                         AFRICA  #AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi       1\n",
      "3     34  52  ablaze               Philadelphia, PA                                                  Crying out for more! Set me ablaze       0\n",
      "4     35  53  ablaze                     London, UK        On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N       0\n",
      "Train set with all observations containing any valid location\n",
      "    index  id keyword                       location                                                                                text  target\n",
      "0     31  48  ablaze                     Birmingham                             @bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C       1\n",
      "1     32  49  ablaze  Est. September 2012 - Bristol                 We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw       0\n",
      "2     33  50  ablaze                         AFRICA  #AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi       1\n",
      "3     34  52  ablaze               Philadelphia, PA                                                  Crying out for more! Set me ablaze       0\n",
      "4     35  53  ablaze                     London, UK        On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N       0\n"
     ]
    }
   ],
   "source": [
    "''' Subset keyword and location columns :: for != to NaN'''\n",
    "train_k = train_df[train_df.keyword.notnull()].reset_index()\n",
    "print('Train set with all observations containing any valid keyword\\n', train_k.head())\n",
    "train_l = train_df[train_df.location.notnull()].reset_index()\n",
    "print('Train set with all observations containing any valid location\\n', train_l.head())\n",
    "train_kl = train_df[(train_df.keyword.notnull()) & (train_df.location.notnull())].reset_index()\n",
    "test_dfx = test_df[(test_df.keyword.notnull()) & (test_df.location.notnull())].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de conocimiento mediante expresiones regulares (RegEx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, realizo en primera instancia una extracción de los conceptos que me parecen o me pueden parecer interesantes desde un punto de vista 'humano', como los hashtags -que visualmente me han parecido señales para predecir si es un desastre real o no- y los usuarios -que a priori, no me han parecido relevantes para posteriormente entrenar los modelos de Machine Learning que he seleccionado, pero que tampoco quiero descartar, prefiriendo que esta decisión la tomen los algoritmos-.\n",
    "\n",
    "Realizo los procesos para ambos sets de datos, el de entrenamiento y el de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                   @bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\n",
      "1                                       We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw\n",
      "2                        #AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi\n",
      "3                                                                        Crying out for more! Set me ablaze\n",
      "4                              On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N\n",
      "                                                       ...                                                 \n",
      "5075                                                    On the bright side I wrecked http://t.co/uEa0txRHYs\n",
      "5076    @widda16 ... He's gone. You can relax. I thought the wife who wrecked her cake was a goner mind ...\n",
      "5077    Three days off from work and they've pretty much all been wrecked hahaha shoutout to my family f...\n",
      "5078          #FX #forex #trading Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/7enNulLKzM\n",
      "5079    @engineshed Great atmosphere at the British Lion gig tonight. Hearing is wrecked. http://t.co/oM...\n",
      "Name: text, Length: 5080, dtype: object\n",
      "length of hashtags:  7613\n",
      "lenght of users:  7613\n"
     ]
    }
   ],
   "source": [
    "''' RegEx '''\n",
    "print(train_kl.text)\n",
    "hashtags = re.findall(r'#.+?\\s', string=str(train_kl.text))\n",
    "hashtags_t = re.findall(r'#.+?\\s', string=str(test_df.text))\n",
    "hashtags = [re.findall(r'#.+?\\s', item) for item in train_df.text]\n",
    "hashtags_t = [re.findall(r'#.+?\\s', item) for item in test_df.text]\n",
    "print('length of hashtags: ', len(hashtags))\n",
    "\n",
    "hashtags = []\n",
    "for item in train_df.text:\n",
    "    loquesea = re.findall(r'#.+?\\s', item)\n",
    "    hashtags.append(loquesea)\n",
    "\n",
    "users = [re.findall(r'@.+?\\s', item) for item in train_df.text]\n",
    "users_t = [re.findall(r'@.+?\\s', item) for item in test_df.text]\n",
    "print('lenght of users: ', len(users))\n",
    "\n",
    "# add hashtags and users to the main dataframe:\n",
    "train_df['hashtags'] = [item[0] if type(item) == 'list' else item for item in hashtags]\n",
    "train_df['users'] = [item[0] if type(item) == 'list' else item for item in users]\n",
    "test_df['hashtags'] = [item[0] if type(item) == 'list' else item for item in hashtags_t]\n",
    "test_df['users'] = [item[0] if type(item) == 'list' else item for item in users_t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, convierto cada tweet en frases con la librería de NLTK, con sent_tokenize. Posteriormente, sobre las mismas frases y para cada observación, transformo cada frase compuesta, en tokens y finalmente en tokens alfanuméricos con el método .isalpha(), ya que sólo me interesan ese tipo de palabras (con o sin números) y no los símbolos (la semántica pura, de cara a extraer el conocimiento necesario para los modelos predictivos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train text sencences:  7613\n",
      "train tokens_per_sent:  7613\n",
      "test text sentences:  3263\n",
      "test tokens_per_sent:  3263\n"
     ]
    }
   ],
   "source": [
    "train_tokenization = 1\n",
    "if train_tokenization == 1:\n",
    "\n",
    "    sentences = [sent_tokenize(sentence) for sentence in train_df.text]\n",
    "    print('train text sencences: ', len(sentences))\n",
    "\n",
    "    tokens_per_sent = [word_tokenize(str(t).lower()) for t in sentences]\n",
    "    print('train tokens_per_sent: ', len(tokens_per_sent))\n",
    "\n",
    "    tokens = []\n",
    "    for sentence in tokens_per_sent:\n",
    "        logic = []\n",
    "        tokens.append(logic)\n",
    "        for word in sentence:\n",
    "            if word.isalpha():\n",
    "                logic.append(word)\n",
    "\n",
    "    train_df['tokens'] = tokens\n",
    "\n",
    "test_tokenization = 1\n",
    "if test_tokenization == 1:\n",
    "\n",
    "    sentences = [sent_tokenize(sentence) for sentence in test_df.text]\n",
    "    print('test text sentences: ', len(sentences))\n",
    "\n",
    "    tokens_per_sent = [word_tokenize(str(t).lower()) for t in sentences]\n",
    "    print('test tokens_per_sent: ', len(tokens_per_sent))\n",
    "\n",
    "    tokens = []\n",
    "    for sentence in tokens_per_sent:\n",
    "        logic = []\n",
    "        tokens.append(logic)\n",
    "        for word in sentence:\n",
    "            if word.isalpha():\n",
    "                logic.append(word)\n",
    "\n",
    "    test_df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>users</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>[#earthquake ]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[deeds, are, the, reason, of, this, earthquake, may, allah, forgive, us, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[fire, near, la, ronge, sask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[all, residents, asked, to, in, place, are, being, notified, by, officers, other, evacuation, or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>[#wildfires ]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[people, receive, wildfires, evacuation, orders, in, california]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>[#Alaska , #wildfires ]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[got, sent, this, photo, from, ruby, alaska, as, smoke, from, wildfires, pours, into, a, school]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                                                                                 text  target                 hashtags users                                                                                               tokens\n",
       "0   1     NaN      NaN                                Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all       1           [#earthquake ]    []                        [deeds, are, the, reason, of, this, earthquake, may, allah, forgive, us, all]\n",
       "1   4     NaN      NaN                                                               Forest fire near La Ronge Sask. Canada       1                       []    []                                                                        [fire, near, la, ronge, sask]\n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...       1                       []    []  [all, residents, asked, to, in, place, are, being, notified, by, officers, other, evacuation, or...\n",
       "3   6     NaN      NaN                                    13,000 people receive #wildfires evacuation orders in California        1            [#wildfires ]    []                                     [people, receive, wildfires, evacuation, orders, in, california]\n",
       "4   7     NaN      NaN             Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school        1  [#Alaska , #wildfires ]    []     [got, sent, this, photo, from, ruby, alaska, as, smoke, from, wildfires, pours, into, a, school]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>users</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[happened, a, terrible, car, crash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, stay safe everyone.</td>\n",
       "      <td>[#earthquake ]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[about, earthquake, is, different, cities, stay, safe, everyone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[is, a, forest, fire, at, spot, pond, geese, are, fleeing, across, the, street, i, can, not, sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>[#Spokane ]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[lighting, spokane, wildfires]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[soudelor, kills, in, china, and, taiwan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                                                                              text        hashtags users                                                                                               tokens\n",
       "0   0     NaN      NaN                                                                Just happened a terrible car crash              []    []                                                                  [happened, a, terrible, car, crash]\n",
       "1   2     NaN      NaN                                  Heard about #earthquake is different cities, stay safe everyone.  [#earthquake ]    []                                     [about, earthquake, is, different, cities, stay, safe, everyone]\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all              []    []  [is, a, forest, fire, at, spot, pond, geese, are, fleeing, across, the, street, i, can, not, sav...\n",
       "3   9     NaN      NaN                                                          Apocalypse lighting. #Spokane #wildfires     [#Spokane ]    []                                                                       [lighting, spokane, wildfires]\n",
       "4  11     NaN      NaN                                                     Typhoon Soudelor kills 28 in China and Taiwan              []    []                                                            [soudelor, kills, in, china, and, taiwan]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, creo un corpus con todos los tokens que me interesan transformados a números (id) con su frecuencia (en tuples en el formato: (id, frecuencia)). \n",
    "\n",
    "Una vez realizada esa tarea, paso para cada una de las observaciones -para cada lista de tokens de cada observacion, y que he añadido previamente como nueva variable a cada dataframe (train y test)-, paso el peso relativo a cada token en cada lista y así ponderar cada valor en cada tweet.\n",
    "\n",
    "Por último, mediante el método pos_tag() de la libreria NLTK, extraigo en listas de tuples, el tipo de token (si es verbo, nombre propio, etc.). Mi objetivo aquí es, mediante el uso de los algoritmos de Machine Learning propuestos más adelante, ver si ese conocimiento, que *aún no he puesto en variables a los train y test sets porque no sé bien cómo (desde el punto de vista lingüistico, no técnico)*. Me está costando mucho esa abstracción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\iojeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tagged tokens:  7613\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(train_df.tokens)\n",
    "corpus = dictionary.doc2bow(list(itertools.chain(*train_df.tokens))) # bow\n",
    "\n",
    "tfidf = TfidfModel([corpus])\n",
    "tfidf_weights = tfidf[corpus]\n",
    "\n",
    "doc_weight = []\n",
    "for tweet in train_df.tokens:\n",
    "    for word in tweet:\n",
    "        dictionary.token2id.get(word)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagged_tokens = [pos_tag(tag) for tag in train_df.tokens]\n",
    "print('Length of tagged tokens: ', len(tagged_tokens))\n",
    "\n",
    "train_df['tagged'] = tagged_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aquí es donde sale mi verdadero problema:** sé como transformar, mediante preprocesado de variables categoricas, transponiendo cada valor único de cada observacion de una variable específica en nombres de variables con valor igual a 1 para aquellas concidencias de par fila-columna y 0 para las restantes (usando el método de One-Hot Enconding => ohe). Lo que no sé, y me ha llevado muchas horas sin avanzar nada, -y me gustaría pedirte ayuda o consejo para cómo actuar en estas situaciones-, es si crear una valor de ohe para cada frase, parabra, sigificado, grupos de n-grams ... no sé que tiene o tendría sentido hacer ... De verdad que me desespera porque me cuesta entenderlo.\n",
    "\n",
    "Posteriormente, pretendía y no me ha dado tiempo y te pido disculpas por ello, transformar el resto de valores que no sean categoricos, y dependiendo del contexto eliminarlos o no ... mediante normalización -valor entre (0, 1)- y estandarización para poder comparar valores en mismas escalas. Para ello pretendía seleccionar los algoritmos de preprocesado: MinMaxScaler() y StandardScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_encoding = 0\n",
    "if categorical_encoding == 1:\n",
    "\n",
    "    ohe = OneHotEncoder()\n",
    "    X_train_df = ohe.fit(train_df.dropna())\n",
    "    y_train = train_df.target\n",
    "\n",
    "    X_test_df = ohe.fit(test_df.dropna())\n",
    "    y_test = test_df.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este último paso, una vez tenga los sets de datos de entrenamiento y test (este último como hold-out set al usar un Cross-Validation previo al training set), evaluar con unos buenos parámetros para cada modelo -**he seleccionado como propones, un algoritmo de Support Vector Machine y también como clásico método, una Regresión Lógistica**- cuál es el mejor algoritmo para este caso de uso.\n",
    "\n",
    "En realidad me hubiera gustado mucho meter un último modelo, y realizar un Ensemble con un Voting Classifier y ver por este mismo método democrático, si quedarnos con 1 <'real disaster'> ó 0 <'not real'> en función de esa votación (pero claro, no he pasado del encoder ...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_method = 0\n",
    "if grid_method == 1:\n",
    "\n",
    "    classification_hold_out = 0\n",
    "    if classification_hold_out == 1:\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        logreg_activate = 0\n",
    "        if logreg_activate == 1:\n",
    "\n",
    "            c_space = np.logspace(-5, 8, 15)\n",
    "            param_grid = {'C': c_space}\n",
    "\n",
    "            logreg = LogisticRegression()\n",
    "            logreg_cv = GridSearchCV(estimator=logreg, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "            logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "            best_params = logreg_cv.best_params_\n",
    "            best_score = logreg_cv.best_score_\n",
    "            print('\\nOptimal tuned params (\\'C\\' in this case): ', best_params)\n",
    "            print('Optimal tuned score (accuracy): ', best_score)\n",
    "\n",
    "        svm_activate = 0\n",
    "        if svm_activate == 1:\n",
    "\n",
    "            svm_model = svm.SVC(kernel='linear')  # instantiate\n",
    "            svm_model.fit(X_train, y_train)\n",
    "            y_pred_svm_model = svm_model.predict(X_test)\n",
    "            print('This is my linear model:\\n', y_pred_svm_model)\n",
    "\n",
    "            print('Accuracy: {:.2f}'.format(metrics.accuracy_score(y_test, y_pred_svm_model)))\n",
    "            print('Precision: {:.2f}'.format(metrics.precision_score(y_test, y_pred_svm_model)))\n",
    "            print('Recall: {:.2f}'.format(metrics.recall_score(y_test, y_pred_svm_model)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
